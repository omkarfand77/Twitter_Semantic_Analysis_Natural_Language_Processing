{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Importing Dependencies***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression     # Logistic Regression\n",
    "from sklearn.svm import SVC                             # Support Vector Machine Algorithm\n",
    "from sklearn.ensemble import RandomForestClassifier     # Random Forest Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier      # K-Nearest Classifier\n",
    "from sklearn.naive_bayes import GaussianNB              # Naives Bayes Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier         # Decision Tree Classifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Text Preprocessing***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Removing URL, Special Characters, Lowering text, Punctuation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Removing URL, Special Characters, Lowering text, Punctuation, Stopwords remove\n",
    "import re #regular expression\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers,\n",
    "    Removing URL's, hashtags, mentions, and special characters.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(\"[0-9\" \"]+\",\" \",text)\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\@\\w+|\\#\", \"\", text)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "clean = lambda x: clean_text(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['figurative', 'irony', 'regular', 'sarcasm'], dtype=object)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"tweet.csv\")\n",
    "df['tweets']=df.tweets.apply(clean)\n",
    "df['class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Remove Stopwords_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        aware dirty step get money staylight staywhite...\n",
       "1             sarcasm people dont understand diy artattack\n",
       "2        iminworkjeremy medsingle dailymail readers sen...\n",
       "3                      wilw get feeling like games sarcasm\n",
       "4        teacherarthurg rweingarten probably missed tex...\n",
       "                               ...                        \n",
       "81403    photo image via heart childhood cool funny sar...\n",
       "81404    never knewi better put universe lolmaybe there...\n",
       "81405    hey wanted say thanks puberty letting apart it...\n",
       "81406    im sure coverage like fox news special hidden ...\n",
       "81407                           wont believe see p sarcasm\n",
       "Name: tweets, Length: 81408, dtype: object"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "df['tweets'] = df['tweets'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "df['tweets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Lemmatization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function to apply Porter stemming to a single word\n",
    "def apply_lemmatization(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(word)\n",
    "\n",
    "# Apply lemmatization to the 'text' column of the DataFrame\n",
    "df['tweets'] = df['tweets'].apply(lambda x: ' '.join([apply_lemmatization(word) for word in word_tokenize(x)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_As our Classification is Multi Class classification. The Categorical Class unique replaced with 0,1,2,3_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replacing for categorical values in class figurative=0, irony=1, regular=2, sarcasm=3\n",
    "df['class'].replace(['figurative', 'irony', 'regular', 'sarcasm'],[0, 1, 2, 3], inplace=True)\n",
    "df['class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Word Embedding With Genism**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pretrained Glove Word2Vec Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhair\\AppData\\Local\\Temp\\ipykernel_16376\\4139260919.py:7: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_model_path, word2vec_output)\n"
     ]
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# Load pre-trained Word2Vec model (GloVe in this case)\n",
    "glove_model_path = 'glove.6B.100d.txt'  # Provide the path to your GloVe model file\n",
    "word2vec_output = \"word2vec.txt\"\n",
    "glove2word2vec(glove_model_path, word2vec_output)\n",
    "\n",
    "# Model\n",
    "glove_model = KeyedVectors.load_word2vec_format(word2vec_output, binary=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Removing of tweets that are not in Glove Vocubulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of removed tweets: 11\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# Assuming tweets need to be tokenized\n",
    "df_tweets = df['tweets'].apply(word_tokenize)\n",
    "\n",
    "features = []\n",
    "removed_tweets = []\n",
    "removed_indices = []\n",
    "\n",
    "for i, tweet_tokens in enumerate(df_tweets):\n",
    "    # Preprocess tweet: remove out-of-vocabulary words\n",
    "    preprocessed_tweet = [word for word in tweet_tokens if word in glove_model.key_to_index]\n",
    "    if preprocessed_tweet:\n",
    "        tweet_embedding = [glove_model.get_vector(word) for word in preprocessed_tweet]\n",
    "        tweet_embedding = np.mean(tweet_embedding, axis=0)  # Average word vectors\n",
    "        features.append(tweet_embedding)\n",
    "    else:\n",
    "        removed_tweets.append(df['tweets'][i])\n",
    "        removed_indices.append(i)\n",
    "\n",
    "# Convert the feature list to a numpy array\n",
    "features = np.array(features)\n",
    "\n",
    "# Create a DataFrame for removed tweets and indices\n",
    "removed_df = pd.DataFrame({'Removed_Tweets': removed_tweets, 'Indices': removed_indices})\n",
    "\n",
    "# Print the number of removed tweets\n",
    "print(f\"Number of removed tweets: {len(removed_tweets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Vectorization using GloVe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Dependant and Inde\n",
    "X = df['tweets']\n",
    "y = df['class']\n",
    "\n",
    "# Tokenize tweets\n",
    "sentences = [nltk.word_tokenize(tweet) for tweet in X]\n",
    "\n",
    "# Vectorization using GloVe\n",
    "def average_word_vectors(tokens, model, num_features):\n",
    "    feature_vector = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    for word in tokens:\n",
    "        if word in model:\n",
    "            nwords += 1\n",
    "            feature_vector = np.add(feature_vector, model[word][:num_features])\n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "    return feature_vector\n",
    "\n",
    "X_vec = [average_word_vectors(tokens, glove_model, 100) for tokens in sentences]\n",
    "\n",
    "X_vec = np.array(X_vec)\n",
    "\n",
    "# Train-Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Min-Max Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Classification Algorithms Applying**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression is: 63.19248249600786\n",
      "Classification Report of Logistic Regression:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.23      0.27      4179\n",
      "           1       0.64      0.73      0.68      4276\n",
      "           2       0.86      0.91      0.88      3696\n",
      "           3       0.63      0.70      0.66      4131\n",
      "\n",
      "    accuracy                           0.63     16282\n",
      "   macro avg       0.61      0.64      0.62     16282\n",
      "weighted avg       0.60      0.63      0.61     16282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Classifier\n",
    "logreg_classifier = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Train the model\n",
    "logreg_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = logreg_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of Logistic Regression is: {accuracy*100}\")\n",
    "print(\"Classification Report of Logistic Regression:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classfier Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Support Vector Classifier is: 65.02272448102198\n",
      "Classification Report Support Vector Classifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.12      0.18      4179\n",
      "           1       0.62      0.82      0.71      4276\n",
      "           2       0.87      0.90      0.88      3696\n",
      "           3       0.62      0.79      0.69      4131\n",
      "\n",
      "    accuracy                           0.65     16282\n",
      "   macro avg       0.61      0.66      0.62     16282\n",
      "weighted avg       0.60      0.65      0.61     16282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM Classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "# Train the model\n",
    "svm_classifier.fit(X_train_scaled, y_train)\n",
    "# Predictions\n",
    "y_pred = svm_classifier.predict(X_test_scaled)\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of Support Vector Classifier is: {accuracy*100}\")\n",
    "print(\"Classification Report Support Vector Classifier:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest Classifier: 49.50865986979486\n",
      "Classification Report of Random Forest Classifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      0.07      0.07      4179\n",
      "           1       0.45      0.48      0.47      4276\n",
      "           2       0.86      0.90      0.88      3696\n",
      "           3       0.52      0.59      0.55      4131\n",
      "\n",
      "    accuracy                           0.50     16282\n",
      "   macro avg       0.48      0.51      0.49     16282\n",
      "weighted avg       0.47      0.50      0.48     16282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of Random Forest Classifier: {accuracy*100}\")\n",
    "print(\"Classification Report of Random Forest Classifier:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree Classifier: 34.332391598083774\n",
      "Classification Report Decision Tree Classifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.17      0.15      4179\n",
      "           1       0.28      0.24      0.26      4276\n",
      "           2       0.74      0.69      0.71      3696\n",
      "           3       0.36      0.32      0.34      4131\n",
      "\n",
      "    accuracy                           0.34     16282\n",
      "   macro avg       0.38      0.35      0.36     16282\n",
      "weighted avg       0.37      0.34      0.35     16282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "dt_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = dt_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of Decision Tree Classifier: {accuracy*100}\")\n",
    "print(\"Classification Report Decision Tree Classifier:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of K-Nearest Classifier: 47.33447979363714\n",
      "Classification Report of K-Nearest Classifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.30      0.26      4179\n",
      "           1       0.44      0.40      0.42      4276\n",
      "           2       0.93      0.73      0.82      3696\n",
      "           3       0.50      0.50      0.50      4131\n",
      "\n",
      "    accuracy                           0.47     16282\n",
      "   macro avg       0.52      0.48      0.50     16282\n",
      "weighted avg       0.51      0.47      0.49     16282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbors (KNN) Classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the model\n",
    "knn_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = knn_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of K-Nearest Classifier: {accuracy*100}\")\n",
    "print(\"Classification Report of K-Nearest Classifier:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naives Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naives Bayes Classifier: 46.26581501044098\n",
      "Classification Report Naives Bayes Classifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.67      0.41      4179\n",
      "           1       0.55      0.30      0.39      4276\n",
      "           2       0.88      0.58      0.70      3696\n",
      "           3       0.65      0.32      0.42      4131\n",
      "\n",
      "    accuracy                           0.46     16282\n",
      "   macro avg       0.59      0.47      0.48     16282\n",
      "weighted avg       0.58      0.46      0.47     16282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Multinomial Naive Bayes Classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train the model\n",
    "nb_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = nb_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of Naives Bayes Classifier: {accuracy*100}\")\n",
    "print(\"Classification Report Naives Bayes Classifier:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Gaussian Naive Bayes: 55.49072595504237\n",
      "Classification Report of Gaussian Naive Bayes:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.21      0.25      4179\n",
      "           1       0.55      0.57      0.56      4276\n",
      "           2       0.71      0.79      0.75      3696\n",
      "           3       0.57      0.68      0.62      4131\n",
      "\n",
      "    accuracy                           0.55     16282\n",
      "   macro avg       0.53      0.56      0.54     16282\n",
      "weighted avg       0.53      0.55      0.54     16282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Gaussian Naive Bayes Classifier\n",
    "gnb_classifier = GaussianNB()\n",
    "\n",
    "# Train the model\n",
    "gnb_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = gnb_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of Gaussian Naive Bayes: {accuracy*100}\")\n",
    "print(\"Classification Report of Gaussian Naive Bayes:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Artificial Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Model building By using ANN kears(sequencial).**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1832/1832 [==============================] - 5s 2ms/step - loss: 1.0652 - accuracy: 0.5016 - val_loss: 0.8859 - val_accuracy: 0.5896\n",
      "Epoch 2/20\n",
      "1832/1832 [==============================] - 4s 2ms/step - loss: 0.8522 - accuracy: 0.6015 - val_loss: 0.9011 - val_accuracy: 0.5888\n",
      "Epoch 3/20\n",
      "1832/1832 [==============================] - 4s 2ms/step - loss: 0.8199 - accuracy: 0.6189 - val_loss: 0.8038 - val_accuracy: 0.6271\n",
      "Epoch 4/20\n",
      "1832/1832 [==============================] - 4s 2ms/step - loss: 0.7984 - accuracy: 0.6284 - val_loss: 0.7877 - val_accuracy: 0.6326\n",
      "Epoch 5/20\n",
      "1832/1832 [==============================] - 5s 2ms/step - loss: 0.7875 - accuracy: 0.6326 - val_loss: 0.7837 - val_accuracy: 0.6343\n",
      "Epoch 6/20\n",
      "1832/1832 [==============================] - 4s 2ms/step - loss: 0.7810 - accuracy: 0.6361 - val_loss: 0.7772 - val_accuracy: 0.6347\n",
      "Epoch 7/20\n",
      "1832/1832 [==============================] - 4s 2ms/step - loss: 0.7772 - accuracy: 0.6371 - val_loss: 0.7924 - val_accuracy: 0.6330\n",
      "Epoch 8/20\n",
      "1832/1832 [==============================] - 4s 2ms/step - loss: 0.7728 - accuracy: 0.6399 - val_loss: 0.7787 - val_accuracy: 0.6406\n",
      "Epoch 9/20\n",
      "1832/1832 [==============================] - 4s 2ms/step - loss: 0.7663 - accuracy: 0.6415 - val_loss: 0.7657 - val_accuracy: 0.6390\n",
      "Epoch 10/20\n",
      "1832/1832 [==============================] - 5s 2ms/step - loss: 0.7635 - accuracy: 0.6423 - val_loss: 0.7610 - val_accuracy: 0.6426\n",
      "Epoch 11/20\n",
      "1832/1832 [==============================] - 5s 3ms/step - loss: 0.7641 - accuracy: 0.6433 - val_loss: 0.7683 - val_accuracy: 0.6427\n",
      "Epoch 12/20\n",
      "1832/1832 [==============================] - 5s 3ms/step - loss: 0.7574 - accuracy: 0.6449 - val_loss: 0.7552 - val_accuracy: 0.6449\n",
      "Epoch 13/20\n",
      "1832/1832 [==============================] - 4s 2ms/step - loss: 0.7487 - accuracy: 0.6496 - val_loss: 0.7564 - val_accuracy: 0.6461\n",
      "Epoch 14/20\n",
      "1832/1832 [==============================] - 5s 3ms/step - loss: 0.7442 - accuracy: 0.6501 - val_loss: 0.7952 - val_accuracy: 0.6272\n",
      "Epoch 15/20\n",
      "1832/1832 [==============================] - 5s 2ms/step - loss: 0.7430 - accuracy: 0.6510 - val_loss: 0.8055 - val_accuracy: 0.6183\n",
      "Epoch 16/20\n",
      "1832/1832 [==============================] - 4s 2ms/step - loss: 0.7395 - accuracy: 0.6515 - val_loss: 0.7505 - val_accuracy: 0.6439\n",
      "Epoch 17/20\n",
      "1832/1832 [==============================] - 5s 3ms/step - loss: 0.7356 - accuracy: 0.6530 - val_loss: 0.7724 - val_accuracy: 0.6386\n",
      "Epoch 18/20\n",
      "1832/1832 [==============================] - 4s 2ms/step - loss: 0.7331 - accuracy: 0.6540 - val_loss: 0.7383 - val_accuracy: 0.6504\n",
      "Epoch 19/20\n",
      "1832/1832 [==============================] - 4s 2ms/step - loss: 0.7305 - accuracy: 0.6552 - val_loss: 0.7707 - val_accuracy: 0.6363\n",
      "Epoch 20/20\n",
      "1832/1832 [==============================] - 5s 2ms/step - loss: 0.7282 - accuracy: 0.6549 - val_loss: 0.7312 - val_accuracy: 0.6525\n",
      "509/509 [==============================] - 1s 2ms/step - loss: 0.7143 - accuracy: 0.6670\n",
      "Accuracy: 0.6669942140579224\n",
      "509/509 [==============================] - 1s 2ms/step\n",
      "Accuracy of Sequential Model is: 66.69942267534701\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.08      0.13      4179\n",
      "           1       0.62      0.85      0.72      4276\n",
      "           2       0.91      0.91      0.91      3696\n",
      "           3       0.62      0.85      0.72      4131\n",
      "\n",
      "    accuracy                           0.67     16282\n",
      "   macro avg       0.62      0.67      0.62     16282\n",
      "weighted avg       0.61      0.67      0.61     16282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Min-Max Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_encoded = to_categorical(y)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vec, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_encoded = to_categorical(y)\n",
    "\n",
    "# Define the Neural Network model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=100, activation='sigmoid'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))  # Adjust the number of output nodes based on your classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=20, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Convert one-hot encoded labels back to original labels\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "_, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Convert one-hot encoded labels back to original labels\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "report = classification_report(y_test_classes, y_pred_classes)\n",
    "\n",
    "print(f\"Accuracy of Sequential Model is: {accuracy*100}\")\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict using Custom Statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "Predicted Class of Tweet: Irony\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_and_predict(text, model, tokenizer, word_vectorizer):\n",
    "    # Step 1: Preprocess the Text\n",
    "    cleaned_text = clean_text(text)\n",
    "    # Step 2: Tokenize and Vectorize the Text\n",
    "    tokens = tokenizer(cleaned_text)\n",
    "    vector = word_vectorizer(tokens, glove_model, 100)  # Adjust the dimensions based on your GloVe model\n",
    "    vector = np.array(vector).reshape(1, -1)  # Reshape to match the expected input shape (1, 200)\n",
    "    # Step 3: Make Predictions\n",
    "    prediction = model.predict(vector)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    class_labels = {0: \"Figurative\",1: \"Irony\",2: \"Regular\",3: \"Sarcasm\"}\n",
    "    predicted_label = class_labels.get(predicted_class, \"Unknown\")\n",
    "    # Print the predicted label\n",
    "    print(f\"Predicted Class of Tweet: {predicted_label}\")\n",
    "    return predicted_class\n",
    "\n",
    "# Example Usage\n",
    "text_input = \"people who call me crazy are usually some of the most #brainwashed #addicted to #propaganda #narratives #irony http://t.co/5diHQRA52G\"\n",
    "predicted_class = preprocess_and_predict(text_input, model, nltk.word_tokenize, average_word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle File For Depoyment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open('model.pkl','wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
